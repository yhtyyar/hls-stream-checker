#!/usr/bin/env python3
import argparse
import json
import logging
import os
import signal
import sys
import tempfile
import time
from collections import deque
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set
from urllib.parse import urljoin

import requests

# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
from data_exporter import OptimizedDataExporter, create_optimized_readme

# -------------------- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è --------------------
PLAYLIST_URL = "https://pl.technettv.com/api/v4/playlist"
PLAYLIST_PARAMS = {
    "tz": "3", "region": "0", "native_region_only": "0", "lang": "en",
    "limit": "0", "page": "1", "epg": "0", "installts": "1756440756",
    "needCategories": "1", "podcasts": "1"
}
X_LHD_AGENT = {
    "generation": 2,
    "sdk": 30,
    "version_name": "1.0.4",
    "version_code": 6,
    "platform": "android",
    "device_id": "5dda2a6f7dcbe35f",
    "name": "samsung+SM-A127F",
    "app": "arabic.tv.watch.online"
}
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "X-LHD-Agent": json.dumps(X_LHD_AGENT, separators=(',', ':')),
    "Content-Type": "application/x-www-form-urlencoded",
}
PLAYLIST_JSON = Path("playlist_streams.json")

# -------------------- –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ --------------------
@dataclass
class SegmentStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
    name: str
    url: str
    success: bool
    size_bytes: int = 0
    download_time: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    error_message: str = ""

@dataclass
class ChannelStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–Ω–∞–ª–∞"""
    channel_name: str = ""
    channel_id: Optional[str] = None
    master_url: str = ""
    variant_url: str = ""
    total_segments: int = 0
    successful_downloads: int = 0
    failed_downloads: int = 0
    total_bytes: int = 0
    total_time: float = 0.0
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    segments: List[SegmentStats] = field(default_factory=list)
    processed_segments: Set[str] = field(default_factory=set)
    
    @property
    def success_rate(self) -> float:
        """–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫"""
        if self.total_segments == 0:
            return 0.0
        return (self.successful_downloads / self.total_segments) * 100
    
    @property
    def avg_download_speed(self) -> float:
        """–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ –≤ MB/s"""
        if self.total_time == 0:
            return 0.0
        return (self.total_bytes / (1024 * 1024)) / self.total_time
    
    @property
    def duration(self) -> float:
        """–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö"""
        end = self.end_time or datetime.now()
        return (end - self.start_time).total_seconds()

@dataclass
class GlobalStats:
    """–ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤"""
    total_channels: int = 0
    completed_channels: int = 0
    total_segments: int = 0
    successful_downloads: int = 0
    failed_downloads: int = 0
    total_bytes: int = 0
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    channels: List[ChannelStats] = field(default_factory=list)
    
    @property
    def overall_success_rate(self) -> float:
        """–û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫"""
        if self.total_segments == 0:
            return 0.0
        return (self.successful_downloads / self.total_segments) * 100
    
    @property
    def duration(self) -> float:
        """–û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö"""
        end = self.end_time or datetime.now()
        return (end - self.start_time).total_seconds()

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
global_stats = GlobalStats()

# -------------------- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ --------------------
logging.basicConfig(
    level=logging.DEBUG,  # –ò–∑–º–µ–Ω–∏–ª–∏ —É—Ä–æ–≤–µ–Ω—å –Ω–∞ DEBUG –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("hls_checker")

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç urllib3
logging.getLogger("urllib3").setLevel(logging.WARNING)

# -------------------- API --------------------
def fetch_playlist() -> Optional[Dict]:
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π payload –∏–∑ main.py
        data = ('subs_packs=%5B%5D&payload=W7ZxsWmx7n6mIy3LgFYDVYFu%2F8pa5iSJctK2rzIpzaFJyq'
               'bPMhjfTANi7fdMC0TXvpGJqInbwhgf%0AT%2BfiBojLZCzimRIvjowGZfdYvlrmoWeWe0ml9%2F'
               '5v6OaaKWYmM9gRJMUet%2FIJTFOvUvrIlgU%2FNUaj%0AyeieV6a3vV6OcJXzKcDEBNtS0JYS8'
               '%2BzK5LmFQvWOxxebn45hcwEkQ17jEsomIdPw4R6h4DgCb5qY%0AdQ0Nra9HwM6tG9s%2FQjBO'
               '9xuG21KkXazegIFLt1pQJpHdzaNiUJcYskSp%2BGa%2Fv%2FlKUjpG7dV5MVkh%0A2O71a9wje'
               'qSaKbmq4D9ZhiTYbRZiEhxdli7idQ%3D%3D%0A')
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ main.py
        ua = ('Mozilla/5.0 (Linux; Android 11; SM-A127F Build/RP1A.200720.012; wv) '
              'AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/139.0.7258.158 '
              'Mobile Safari/537.36')
        
        headers = {
            "Host": "pl.technettv.com",
            "User-Agent": ua,
            "x-lhd-agent": json.dumps(X_LHD_AGENT, separators=(',', ':')),
            "x-token": "null",
            "cache-control": "no-cache",
            "content-type": "application/x-www-form-urlencoded"
        }
        
        logger.debug(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ {PLAYLIST_URL}")
        logger.debug(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {PLAYLIST_PARAMS}")
        logger.debug(f"–ó–∞–≥–æ–ª–æ–≤–∫–∏: {headers}")
        
        r = requests.post(
            PLAYLIST_URL, 
            params=PLAYLIST_PARAMS, 
            data=data, 
            headers=headers, 
            timeout=20
        )
        r.raise_for_status()
        
        response_data = r.json()
        logger.debug(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: {r.status_code}")
        logger.debug(f"–†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(r.text)} –±–∞–π—Ç")
        logger.debug(f"–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞: {type(response_data)}")
        
        if isinstance(response_data, dict):
            logger.debug(f"–ö–ª—é—á–∏ –≤ –æ—Ç–≤–µ—Ç–µ: {list(response_data.keys())}")
        
        return response_data
        
    except requests.exceptions.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞: {e}")
        if hasattr(e, 'response') and e.response is not None:
            logger.error(f"–ö–æ–¥ –æ—Ç–≤–µ—Ç–∞: {e.response.status_code}")
            logger.error(f"–¢–µ–ª–æ –æ—Ç–≤–µ—Ç–∞: {e.response.text[:500]}...")
        return None
    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        return None
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return None

def save_channels(api_json: Dict):
    channels = []
    if not api_json:
        logger.error("–ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç API")
        return
        
    items = api_json.get("channels", [])
    if not items:
        logger.error("–í –æ—Ç–≤–µ—Ç–µ API –Ω–µ—Ç —Å–ø–∏—Å–∫–∞ –∫–∞–Ω–∞–ª–æ–≤")
        return
        
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(items)} –∫–∞–Ω–∞–ª–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ API")
    for item in items:
        if not item:
            continue
            
        stream_url = ""
        if "stream" in item and isinstance(item["stream"], dict):
            stream_url = item["stream"].get("common", "").strip()
            
        channel = {
            "our_id": item.get("our_id"),
            "name_ru": item.get("name_ru") or item.get("title") or "",
            "stream_common": stream_url,
            "url": (item.get("url") or "").strip(),
        }
        channels.append(channel)
    
    if not channels:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ API")
        return
        
    with open(PLAYLIST_JSON, "w", encoding="utf-8") as f:
        json.dump(channels, f, ensure_ascii=False, indent=2)
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(channels)} –∫–∞–Ω–∞–ª–æ–≤ –≤ {PLAYLIST_JSON}")
    
    # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–π –∫–∞–Ω–∞–ª –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    if channels:
        logger.debug(f"–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞: {json.dumps(channels[0], ensure_ascii=False)}")

def load_channels() -> List[Dict]:
    if not PLAYLIST_JSON.exists():
        return []
    with open(PLAYLIST_JSON, "r", encoding="utf-8") as f:
        return json.load(f)

# -------------------- M3U8 --------------------
def parse_master(text: str, base_url: str) -> List[Dict]:
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    variants = []
    i = 0
    while i < len(lines):
        if lines[i].startswith("#EXT-X-STREAM-INF:"):
            attrs = {}
            for part in lines[i].split(":", 1)[1].split(","):
                if "=" in part:
                    k, v = part.split("=", 1)
                    attrs[k] = v.strip().strip('"')
            uri = lines[i+1] if i+1 < len(lines) else None
            bw = int(attrs.get("BANDWIDTH", 0))
            res = (0, 0)
            if "RESOLUTION" in attrs:
                try:
                    w, h = attrs["RESOLUTION"].split("x")
                    res = (int(w), int(h))
                except: pass
            if uri:
                variants.append({"bw": bw, "res": res, "uri": urljoin(base_url, uri)})
            i += 2
        else:
            i += 1
    return variants

def best_variant(variants: List[Dict]) -> Optional[str]:
    if not variants: return None
    return max(variants, key=lambda v: (v["bw"], v["res"]))["uri"]

# -------------------- –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ç–æ–∫–∞ --------------------
class HLSStreamChecker:
    def __init__(self, url: str, channel_stats: Optional[ChannelStats] = None):
        self.url = url
        self.headers = HEADERS
        self.running = True
        self.stats = channel_stats or ChannelStats()
        self.segment_buffer = deque(maxlen=100)  # –ë—É—Ñ–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        
        signal.signal(signal.SIGINT, self._stop)
        signal.signal(signal.SIGTERM, self._stop)

    def _stop(self, *_):
        self.running = False

    def fetch_text(self, url: str) -> Optional[str]:
        try:
            r = requests.get(url, headers=self.headers, timeout=10)
            r.raise_for_status()
            return r.text
        except: return None

    def parse_media(self, text: str) -> List[str]:
        lines = [ln.strip() for ln in text.splitlines() if ln and not ln.startswith("#")]
        return lines

    def download_segment(self, url: str) -> tuple[bool, SegmentStats]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        segment_name = url.split('/')[-1]
        start_time = time.time()
        
        segment_stats = SegmentStats(
            name=segment_name,
            url=url,
            success=False,
            timestamp=datetime.now()
        )
        
        try:
            with requests.get(url, headers=self.headers, timeout=10, stream=True) as r:
                r.raise_for_status()
                tmp = tempfile.NamedTemporaryFile(delete=False)
                total_size = 0
                
                for chunk in r.iter_content(1024*64):
                    if chunk: 
                        tmp.write(chunk)
                        total_size += len(chunk)
                
                tmp.close()
                download_time = time.time() - start_time
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                segment_stats.success = True
                segment_stats.size_bytes = total_size
                segment_stats.download_time = download_time
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—è–º–∏
                size_mb = total_size / (1024 * 1024)
                speed_mbps = size_mb / download_time if download_time > 0 else 0
                logger.info(f"‚úÖ {segment_name} - {size_mb:.2f} MB, –≤—Ä–µ–º—è: {download_time:.2f}s, —Å–∫–æ—Ä–æ—Å—Ç—å: {speed_mbps:.2f} MB/s")
                
                os.unlink(tmp.name)
                return True, segment_stats
                
        except requests.exceptions.RequestException as e:
            error_msg = f"–û—à–∏–±–∫–∞ HTTP: {e}"
            segment_stats.error_message = error_msg
            logger.error(f"‚ùå {segment_name} - {error_msg}")
        except Exception as e:
            error_msg = f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}"
            segment_stats.error_message = error_msg
            logger.error(f"‚ùå {segment_name} - {error_msg}")
        
        segment_stats.download_time = time.time() - start_time
        return False, segment_stats
    
    def _print_intermediate_stats(self):
        """–ü–µ—á–∞—Ç–∞–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        if self.stats.total_segments == 0:
            return
            
        logger.info(f"üìà –ü–†–û–ú–ï–ñ–£–¢–û–ß–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"üìÑ –°–µ–≥–º–µ–Ω—Ç–æ–≤: {self.stats.successful_downloads}/{self.stats.total_segments} "
                   f"({self.stats.success_rate:.1f}% —É—Å–ø–µ—à–Ω–æ)")
        if self.stats.total_bytes > 0:
            logger.info(f"üì° –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {self.stats.total_bytes / (1024 * 1024):.2f} MB")
        if self.stats.avg_download_speed > 0:
            logger.info(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {self.stats.avg_download_speed:.2f} MB/s")
        logger.info(f"‚è± –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {self.stats.duration:.1f} —Å–µ–∫—É–Ω–¥")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏
        recent_errors = [s for s in self.stats.segments[-10:] if not s.success]
        if recent_errors:
            logger.info(f"‚ö†Ô∏è –ü–æ—Å–ª–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏: {len(recent_errors)} –∏–∑ 10")
    
    def _print_final_stats(self):
        """–ü–µ—á–∞—Ç–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–Ω–∞–ª—É"""
        logger.info("=" * 70)
        logger.info("üìÅ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–ê–ù–ê–õ–ê")
        
        if self.stats.channel_name:
            logger.info(f"üì∫ –ö–∞–Ω–∞–ª: {self.stats.channel_name}")
        logger.info(f"üîó URL: {self.url}")
        logger.info(f"üìà –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {self.stats.total_segments}")
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫: {self.stats.successful_downloads}")
        logger.info(f"‚ùå –ù–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫: {self.stats.failed_downloads}")
        
        if self.stats.total_segments > 0:
            logger.info(f"üéØ –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {self.stats.success_rate:.1f}%")
        
        if self.stats.total_bytes > 0:
            total_mb = self.stats.total_bytes / (1024 * 1024)
            logger.info(f"üì° –û–±—â–∏–π –æ–±—ä–µ–º: {total_mb:.2f} MB")
            
        if self.stats.avg_download_speed > 0:
            logger.info(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {self.stats.avg_download_speed:.2f} MB/s")
            
        logger.info(f"‚è± –û–±—â–∞—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {self.stats.duration:.1f} —Å–µ–∫—É–Ω–¥")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ –æ—à–∏–±–æ–∫
        if self.stats.failed_downloads > 0:
            logger.info(f"‚ö†Ô∏è –û—à–∏–±–∫–∏:")
            error_counts = {}
            for seg in self.stats.segments:
                if not seg.success and seg.error_message:
                    error_counts[seg.error_message] = error_counts.get(seg.error_message, 0) + 1
            
            for error, count in error_counts.items():
                logger.info(f"   {error}: {count} —Ä–∞–∑")
        
        logger.info("=" * 70)

    def run_for_duration(self, seconds: int):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
        logger.info("=" * 70)
        logger.info(f"üöÄ –ù–ê–ß–ò–ù–ê–Æ –ü–†–û–í–ï–†–ö–£ HLS –ü–û–¢–û–ö–ê")
        logger.info(f"üì∫ URL: {self.url}")
        logger.info(f"‚è± –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {seconds} —Å–µ–∫—É–Ω–¥")
        logger.info("‚èπ –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")
        logger.info("=" * 70)
        
        self.stats.start_time = datetime.now()
        end_time = time.time() + seconds
        last_stats_update = time.time()
        stats_update_interval = 10.0  # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
        
        while self.running and time.time() < end_time:
            manifest = self.fetch_text(self.url)
            if not manifest:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–∞–Ω–∏—Ñ–µ—Å—Ç. –ü–æ–≤—Ç–æ—Ä—è—é –ø–æ–ø—ã—Ç–∫—É...")
                time.sleep(1)
                continue
            
            segments = self.parse_media(manifest)
            new_segments = [seg for seg in segments if seg not in self.stats.processed_segments]
            
            for seg in new_segments:
                if time.time() >= end_time or not self.running:
                    break
                    
                self.stats.processed_segments.add(seg)
                full_url = urljoin(self.url, seg)
                
                success, segment_stats = self.download_segment(full_url)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self.stats.total_segments += 1
                self.stats.segments.append(segment_stats)
                
                if success:
                    self.stats.successful_downloads += 1
                    self.stats.total_bytes += segment_stats.size_bytes
                    self.stats.total_time += segment_stats.download_time
                    self.segment_buffer.append(segment_stats)
                else:
                    self.stats.failed_downloads += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                global_stats.total_segments += 1
                if success:
                    global_stats.successful_downloads += 1
                    global_stats.total_bytes += segment_stats.size_bytes
                else:
                    global_stats.failed_downloads += 1
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                current_time = time.time()
                if current_time - last_stats_update >= stats_update_interval:
                    self._print_intermediate_stats()
                    last_stats_update = current_time
                
                time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
        
        self.stats.end_time = datetime.now()
        self._print_final_stats()
        logger.info("‚èπ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

# -------------------- –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä --------------------
def run_checks(channels: List[Dict], minutes: int, count: str, args):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞–Ω–∞–ª–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å—é"""
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã –ø—Ä–æ–≤–µ—Ä—è—Ç—å
    if count == "all": 
        selected = channels
    else:
        try: 
            selected = channels[:int(count)]
        except: 
            selected = channels[:1]
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    global_stats.total_channels = len(selected)
    global_stats.start_time = datetime.now()
    
    logger.info("=" * 80)
    logger.info("üéÜ –ù–ê–ß–ê–õ–û –ü–†–û–í–ï–†–ö–ò HLS –ö–ê–ù–ê–õ–û–í")
    logger.info(f"üìÑ –í—Å–µ–≥–æ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(selected)}")
    logger.info(f"‚è± –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {minutes} –º–∏–Ω—É—Ç")
    logger.info("=" * 80)

    for i, ch in enumerate(selected, 1):
        try:
            # –ü–æ–ª—É—á–∞–µ–º URL –º–∞—Å—Ç–µ—Ä –ø–ª–µ–π–ª–∏—Å—Ç–∞
            master = ch.get("stream_common") or ch.get("url")
            if not master: 
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é –∫–∞–Ω–∞–ª {ch.get('name_ru', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')} - –Ω–µ—Ç URL")
                continue
                
            # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–Ω–∞–ª–∞
            channel_stats = ChannelStats(
                channel_name=ch.get('name_ru', ''),
                channel_id=ch.get('our_id'),
                master_url=master
            )
            global_stats.channels.append(channel_stats)
            
            logger.info("\n" + "="*60)
            logger.info(f"üì∫ –ö–∞–Ω–∞–ª {i}/{len(selected)}: {channel_stats.channel_name}")
            logger.info(f"üîó Master URL: {master}")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–∞—Å—Ç–µ—Ä –ø–ª–µ–π–ª–∏—Å—Ç –∏ –Ω–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
            try:
                response = requests.get(master, headers=HEADERS, timeout=10)
                response.raise_for_status()
                txt = response.text
            except Exception as e:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–∞—Å—Ç–µ—Ä –ø–ª–µ–π–ª–∏—Å—Ç: {e}")
                continue
                
            variants = parse_master(txt, master)
            variant = best_variant(variants) or master
            channel_stats.variant_url = variant
            
            logger.info(f"‚öôÔ∏è –ù–∞–π–¥–µ–Ω–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤: {len(variants)}")
            logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: {variant}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
            checker = HLSStreamChecker(variant, channel_stats)
            checker.run_for_duration(minutes * 60)
            
            global_stats.completed_channels += 1
            
        except KeyboardInterrupt:
            logger.info("‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–∞–Ω–∞–ª–∞: {e}")
            continue
    
    # –ü–µ—á–∞—Ç–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    global_stats.end_time = datetime.now()
    print_global_stats()
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    if not getattr(args, 'no_export', False):
        export_session_data(global_stats)

# -------------------- –ì–ª–æ–±–∞–ª—å–Ω–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å --------------------
def print_global_stats():
    """–ü–µ—á–∞—Ç–∞–µ—Ç –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –∫–∞–Ω–∞–ª–∞–º"""
    logger.info("\n" + "=" * 80)
    logger.info("üèÜ –û–ë–©–ê–Ø –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    logger.info("=" * 80)
    
    # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    logger.info(f"üì∫ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤: {global_stats.total_channels}")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {global_stats.completed_channels}")
    logger.info(f"‚è± –û–±—â–∞—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {global_stats.duration:.1f} —Å–µ–∫—É–Ω–¥")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
    logger.info(f"üìà –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {global_stats.total_segments}")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫: {global_stats.successful_downloads}")
    logger.info(f"‚ùå –ù–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫: {global_stats.failed_downloads}")
    
    if global_stats.total_segments > 0:
        logger.info(f"üéØ –û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {global_stats.overall_success_rate:.1f}%")
    
    if global_stats.total_bytes > 0:
        total_mb = global_stats.total_bytes / (1024 * 1024)
        logger.info(f"üì° –û–±—â–∏–π –æ–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö: {total_mb:.2f} MB")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º
    if global_stats.channels:
        logger.info("\nüìâ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–ê–ù–ê–õ–ê–ú:")
        logger.info("-" * 80)
        
        for channel in global_stats.channels:
            if channel.total_segments > 0:
                logger.info(
                    f"üì∫ {channel.channel_name or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'}: "
                    f"{channel.successful_downloads}/{channel.total_segments} "
                    f"({channel.success_rate:.1f}%) - "
                    f"{channel.total_bytes / (1024 * 1024):.1f} MB - "
                    f"{channel.duration:.1f}s"
                )
        
        # –ù–∞–∏–ª—É—á—à–∏–µ –∏ –Ω–∞–∏—Ö—É–¥—à–∏–µ –∫–∞–Ω–∞–ª—ã
        channels_with_data = [ch for ch in global_stats.channels if ch.total_segments > 0]
        if len(channels_with_data) > 1:
            best_channel = max(channels_with_data, key=lambda x: x.success_rate)
            worst_channel = min(channels_with_data, key=lambda x: x.success_rate)
            
            logger.info("\nüèÖ –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            logger.info(f"ü•á –õ—É—á—à–∏–π –∫–∞–Ω–∞–ª: {best_channel.channel_name} ({best_channel.success_rate:.1f}%)")
            logger.info(f"ü•â –ü—Ä–æ–±–ª–µ–º–Ω—ã–π –∫–∞–Ω–∞–ª: {worst_channel.channel_name} ({worst_channel.success_rate:.1f}%)")
    
    # –û–±—â–∏–µ –æ—à–∏–±–∫–∏
    all_errors = {}
    for channel in global_stats.channels:
        for segment in channel.segments:
            if not segment.success and segment.error_message:
                all_errors[segment.error_message] = all_errors.get(segment.error_message, 0) + 1
    
    if all_errors:
        logger.info("\n‚ö†Ô∏è –û–ë–©–ò–ï –û–®–ò–ë–ö–ò:")
        for error, count in sorted(all_errors.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"   {error}: {count} —Ä–∞–∑")
    
    logger.info("\n" + "=" * 80)
    logger.info("üéâ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    logger.info("=" * 80)


def export_session_data(global_stats):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ—Å—Å–∏–∏ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
    try:
        # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä
        exporter = OptimizedDataExporter(
            session_start=global_stats.start_time,
            session_end=global_stats.end_time
        )
        
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        exported_files = exporter.export_final_statistics(global_stats)
        
        # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π README –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
        readme_path = Path("data") / "README.md"
        if not readme_path.exists():
            create_optimized_readme()
        
        logger.info("\nüéÜ –≠–ö–°–ü–û–†–¢ –§–ò–ù–ê–õ–¨–ù–û–ô –°–¢–ê–¢–ò–°–¢–ò–ö–ò –£–°–ü–ï–®–ù–û!")
        logger.info("üìà CSV —Ñ–∞–π–ª—ã - –¥–ª—è –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –≤ Excel")
        logger.info("üöÄ JSON —Ñ–∞–π–ª—ã - –¥–ª—è API –∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞")
        
        return exported_files
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

# -------------------- CLI --------------------
def main():
    p = argparse.ArgumentParser(description="HLS Stream Checker - –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ HLS –ø–æ—Ç–æ–∫–æ–≤")
    p.add_argument("--count", default="1", help="1,10,20,all (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1)")
    p.add_argument("--minutes", type=int, default=1, help="–í—Ä–µ–º—è —Ç–µ—Å—Ç–∞ (–º–∏–Ω—É—Ç—ã)")
    p.add_argument("--refresh", action="store_true", help="–û–±–Ω–æ–≤–∏—Ç—å –ø–ª–µ–π–ª–∏—Å—Ç")
    p.add_argument("--no-export", action="store_true", help="–û—Ç–∫–ª—é—á–∏—Ç—å —ç–∫—Å–ø–æ—Ä—Ç –≤ CSV/JSON")
    args = p.parse_args()

    if args.refresh or not PLAYLIST_JSON.exists():
        data = fetch_playlist()
        if not data: sys.exit(1)
        save_channels(data)

    channels = load_channels()
    run_checks(channels, args.minutes, args.count, args)

if __name__ == "__main__":
    main()
